{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Set the path to the uploaded ZIP file\n",
        "zip_path = '/content/Clutch.v3i.yolov11.zip'  # Update this if your file has a different name\n",
        "extract_path = '/content/dataset'  # Destination folder\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Dataset extracted to {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTvz_UWYC2GJ",
        "outputId": "066d85fd-6a3d-4a36-ac3c-fb3a35ba4765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted to /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "import os\n",
        "import cv2\n",
        "from glob import glob\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Example YOLO -> Albumentations conversion helper\n",
        "def yolo_to_albumentations(box, img_w, img_h):\n",
        "    x_center, y_center, width, height = box\n",
        "    x_min = (x_center - width / 2) * img_w\n",
        "    y_min = (y_center - height / 2) * img_h\n",
        "    x_max = (x_center + width / 2) * img_w\n",
        "    y_max = (y_center + height / 2) * img_h\n",
        "    return [x_min, y_min, x_max, y_max]\n",
        "\n",
        "def albumentations_to_yolo(box, img_w, img_h):\n",
        "    x_min, y_min, x_max, y_max = box\n",
        "    x_center = ((x_min + x_max) / 2) / img_w\n",
        "    y_center = ((y_min + y_max) / 2) / img_h\n",
        "    width = (x_max - x_min) / img_w\n",
        "    height = (y_max - y_min) / img_h\n",
        "    return [x_center, y_center, width, height]\n",
        "\n",
        "# Albumentations transform with bbox support\n",
        "transform = A.Compose([\n",
        "    A.Rotate(limit=15, p=0.7),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.GaussNoise(p=0.3)\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
        "\n",
        "# Paths\n",
        "image_dir = \"/content/dataset/train/images\"\n",
        "label_dir = \"/content/dataset/train/labels\"\n",
        "output_img_dir = \"/content/dataset/augmented_images\"\n",
        "output_lbl_dir = \"/content/dataset/augmented_labels\"\n",
        "\n",
        "os.makedirs(output_img_dir, exist_ok=True)\n",
        "os.makedirs(output_lbl_dir, exist_ok=True)\n",
        "\n",
        "# Process each image\n",
        "for img_path in glob(os.path.join(image_dir, \"*.jpg\")):\n",
        "    img = cv2.imread(img_path)\n",
        "    h, w, _ = img.shape\n",
        "    filename = os.path.basename(img_path).replace('.jpg', '')\n",
        "\n",
        "    label_path = os.path.join(label_dir, f\"{filename}.txt\")\n",
        "    if not os.path.exists(label_path): continue\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        lines = f.read().splitlines()\n",
        "\n",
        "    boxes = []\n",
        "    class_labels = []\n",
        "    for line in lines:\n",
        "        cls, x, y, bw, bh = map(float, line.strip().split())\n",
        "        box = yolo_to_albumentations([x, y, bw, bh], w, h)\n",
        "        boxes.append(box)\n",
        "        class_labels.append(int(cls))\n",
        "\n",
        "    for i in range(10):  # 5 augmentations per image\n",
        "        augmented = transform(image=img, bboxes=boxes, class_labels=class_labels)\n",
        "        aug_img = augmented[\"image\"]\n",
        "        aug_boxes = augmented[\"bboxes\"]\n",
        "        aug_labels = augmented[\"class_labels\"]\n",
        "\n",
        "        out_img_name = f\"{filename}_aug_{i}.jpg\"\n",
        "        out_lbl_name = f\"{filename}_aug_{i}.txt\"\n",
        "\n",
        "        cv2.imwrite(os.path.join(output_img_dir, out_img_name), aug_img)\n",
        "\n",
        "        with open(os.path.join(output_lbl_dir, out_lbl_name), \"w\") as f:\n",
        "            for box, cls in zip(aug_boxes, aug_labels):\n",
        "                yolo_box = albumentations_to_yolo(box, w, h)\n",
        "                f.write(f\"{cls} {' '.join(map(str, yolo_box))}\\n\")\n"
      ],
      "metadata": {
        "id": "zSs2irlEQppE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "# Original folders\n",
        "orig_img_dir = \"/content/dataset/train/images\"\n",
        "orig_lbl_dir = \"/content/dataset/train/labels\"\n",
        "\n",
        "# Augmented folders\n",
        "aug_img_dir = \"/content/dataset/augmented_images\"\n",
        "aug_lbl_dir = \"/content/dataset/augmented_labels\"\n",
        "\n",
        "# Combined output folders\n",
        "combined_img_dir = \"/content/dataset/combined/train/images\"\n",
        "combined_lbl_dir = \"/content/dataset/combined/train/labels\"\n",
        "\n",
        "# Make sure combined folders exist\n",
        "os.makedirs(combined_img_dir, exist_ok=True)\n",
        "os.makedirs(combined_lbl_dir, exist_ok=True)\n",
        "\n",
        "# Copy original images\n",
        "for file in glob(os.path.join(orig_img_dir, \"*.jpg\")):\n",
        "    shutil.copy(file, combined_img_dir)\n",
        "\n",
        "# Copy original labels\n",
        "for file in glob(os.path.join(orig_lbl_dir, \"*.txt\")):\n",
        "    shutil.copy(file, combined_lbl_dir)\n",
        "\n",
        "# Copy augmented images\n",
        "for file in glob(os.path.join(aug_img_dir, \"*.jpg\")):\n",
        "    shutil.copy(file, combined_img_dir)\n",
        "\n",
        "# Copy augmented labels\n",
        "for file in glob(os.path.join(aug_lbl_dir, \"*.txt\")):\n",
        "    shutil.copy(file, combined_lbl_dir)\n",
        "\n",
        "print(\"✅ All original + augmented data combined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQvp2XEpVA_X",
        "outputId": "c50b5a2b-d5ef-4122-a2da-09eacee4fe17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All original + augmented data combined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Zip the combined dataset folder\n",
        "shutil.make_archive(\"/content/combined_dataset\", 'zip', \"/content/dataset/combined\")\n",
        "\n",
        "print(\"✅ Zipped as /content/combined_dataset.zip\")"
      ],
      "metadata": {
        "id": "dWUhAKLSVm-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d32117-5c9c-4bca-e43d-ef6096076ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Zipped as /content/combined_dataset.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9cBPWc9eMHpE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}